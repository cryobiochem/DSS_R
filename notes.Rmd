---
title: '**Notes and scripts**'
author: "cryobiochem"
output:
  html_notebook: default
  pdf_document: default
---

## 1 The Data Scientist's Toolbox



## 2. R Programming
### Basics
```{r}
# explicit coercion
as.numeric(x)
as.logical(x)
as.character(x)
as.complex(x)

# data types
vector("class", n)
list()
array(data, dim, dimnames)
matrix(values, nrow=n, ncol=m)
data.frame(vector, matrix)
data.table()
factor(c("a", "b"), levels = c("1", "2"))

# joins
rbind(x,y)
cbind(x,y)

# explore
dim(df)
dimnames(df)
names(df)
head(df)
tail(df)
summary(df)
str(df)
colMeans(df)
rowMeans(df)

# sequences
seq(1, 20, by=0.5)
rep(0, times=40)
replicate(n, expression)
length()
seq_along(vector)


# comparison
identical(x,y) # exactly equal
all.equal(x,y) # near equal

# counting (with dplyr package)
n()
n_distinct()
```


### Split-Apply-Combine
```{r}
split(x,f,drop=FALSE)
interactions(x,y)
apply(array, margin, function)
lapply(list, function)
sapply(list, function)
vapply(list, function, FUN.VALUE = type, ...)
tapply(vector, index, function)
mapply(function, ...)
aggregate(...) # compute summary of multiple subsets (multiple t-apply)
```


### Data Simulation
```{r}
rbinom(n = , size=, prob=)
rnorm(n, mean = m, sd = s)
dnorm(x, mean = 0, sd = 1, log = FALSE)
pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
rpois(n, lambda)
rpois(n, r)
ppois(n, r)
```


### Base Graphics
```{r}
plot(x, y, xlab, ylab, main, sub, col, pch, xlim, ylim)
boxplot(x~y, data=d)
hist(x, breaks)
```



### Datetime
```{r}
Date
POSIXct
POSIXlt

Sys.Date()
Sys.time()
weekdays(date)
months(date)
quarters(date)
strptime(string, "%B %d, %Y %H:%M")
difftime(time1, time2, units = 'days')
```


### **pollutantmean.R**
```{r}
# PROGRAMMING ASSIGMENT 1: AIR POLLUTION
# PART 1
# Write a function named 'pollutantmean' that calculates the mean of a pollutant
# (sulfate or nitrate) across a specified list of monitors. The function 
# 'pollutantmean' takes three arguments: 'directory', 'pollutant', and 'id'. Given
# a vector monitor ID numbers, 'pollutantmean' reads that monitors' particulate 
# matter data from the directory specified in the 'directory' argument and returns
# the mean of the pollutant across all of the monitors, ignoring any missing 
# values coded as NA. A prototype of the function is as follows

pollutantmean <- function(directory, pollutant, id = 1:332) {
    # Get full path of the specsdata folder
    directory <- paste(getwd(),"/",directory,"/",sep="")
    
    # Aux variables
    file_list <- list.files(directory)
    data <- NA
    #For each id passed as parameter:
    for (i in id) {
        #Read the file,
        file_dir <- paste(directory,file_list[i],sep="")
        file_data <- read.csv(file_dir)
        
        # accumulate the data
        data <- rbind(data,file_data)
    }
    # Calculate the mean and return it
    mean(data[[pollutant]],na.rm = TRUE)
}

# >============== TEST SCENARIOS ==============<

test1 <- pollutantmean("specdata","sulfate",id=1:10) 
test1
#4.064
test2 <- pollutantmean("specdata","nitrate",id=70:72) 
test2
#1.706
test3 <- pollutantmean("specdata","nitrate",id=23) 
test3
#1.281
```

### **complete.R**
```{r}
# PROGRAMMING ASSIGMENT 1: AIR POLLUTION
# PART 2
# Write a function that reads a directory full of files and reports the number of 
# completely observed cases in each data file. The function should return a data 
# frame where the first column is the name of the file and the second column is 
# the number of complete cases. A prototype of this function follows


complete <- function(directory, id = 1:332) {
    # Get full path of the specsdata folder
    directory <- paste(getwd(),"/","specdata","/",sep="")
    
    # Aux variables
    file_list <- list.files(directory)
    ids <- vector()
    nobs <- vector()
    
    #For each id passed as parameter:
    for (i in id) {
        # Read the file,
        file_dir <- paste(directory,file_list[i],sep="")
        file_data <- read.csv(file_dir)
        
        # acumulate ids and nobs values in the vectors    
        ids = c(ids,i)
        nobs = c(nobs,sum(complete.cases(file_data)))        
    }
    # Finally, Create the data frame using the vectors and return it
    data.frame(id = ids, nobs = nobs)
}


# >============== TEST SCENARIOS ==============<

test1 <- complete("specdata", 1)
test1
##   id nobs
## 1  1  117
test2 <- complete("specdata", c(2, 4, 8, 10, 12))
test2
##   id nobs
## 1  2 1041
## 2  4  474
## 3  8  192
## 4 10  148
## 5 12   96
test3 <- complete("specdata", 30:25)
test3
##   id nobs
## 1 30  932
## 2 29  711
## 3 28  475
## 4 27  338
## 5 26  586
## 6 25  463
test4 <- complete("specdata", 3)
test4
##   id nobs
## 1  3  243
```

### **corr.R**
```{r}
# PROGRAMMING ASSIGMENT 1: AIR POLLUTION
# PART 3
# Write a function that takes a directory of data files and a threshold for 
# complete cases and calculates the correlation between sulfate and nitrate for 
# monitor locations where the number of completely observed cases (on all 
# variables) is greater than the threshold. The function should return a vector 
# of correlations for the monitors that meet the threshold requirement. If no
# monitors meet the threshold requirement, then the function should return a 
# numeric vector of length 0.

source("complete.R")

corr <- function(directory, threshold = 0) {
    # Get full path of the specsdata folder
    directory <- paste(getwd(),"/",directory,"/",sep="")    
    
    #Get observations and filter by threshold
    observations <- complete(directory)
    filtered_observations = subset(observations,observations$nobs > threshold)
        
    # Aux variables
    file_list <- list.files(directory)
    correlation <- vector()
    
    # For each id in filtered observations:
    for (i in filtered_observations$id) {
        # Read the file,
        file_dir <- paste(directory,file_list[i],sep="")
        file_data <- read.csv(file_dir)
        # remove NA,
        file_data <- subset(file_data,complete.cases(file_data))        
        # and calculate the cor and accumulate it in the corellation vector.
        correlation <- c(correlation,cor(file_data$nitrate,file_data$sulfate))    
    }
    #Finally, return the vector
    correlation
}

# >============== TEST SCENARIOS ==============<

#source("corr.R")
#source("complete.R")
cr <- corr("specdata", 150)
head(cr)
## [1] -0.01896 -0.14051 -0.04390 -0.06816 -0.12351 -0.07589
summary(cr)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -0.2110 -0.0500  0.0946  0.1250  0.2680  0.7630
cr <- corr("specdata", 400)
head(cr)
## [1] -0.01896 -0.04390 -0.06816 -0.07589  0.76313 -0.15783
summary(cr)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -0.1760 -0.0311  0.1000  0.1400  0.2680  0.7630
cr <- corr("specdata", 5000)
summary(cr)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 
length(cr)
## [1] 0
cr <- corr("specdata")
summary(cr)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -1.0000 -0.0528  0.1070  0.1370  0.2780  1.0000
length(cr)
## [1] 323
```
### **cachevector.R**
```{r}
makeVector <- function(x = numeric()) {
    m <- NULL
    set <- function(y) {
        x <<- y
        m <<- NULL
    }
    get <- function() x
    setmean <- function(mean) m <<- mean
    getmean <- function() m
    list(set = set, get = get,
         setmean = setmean,
         getmean = getmean)
}

cachemean <- function(x, ...) {
    m <- x$getmean()
    if(!is.null(m)) {
        message("getting cached data")
        return(m)
    }
    data <- x$get()
    m <- mean(data, ...)
    x$setmean(m)
    m
}

# >============ EXAMPLES ==============<

a <- makeVector(c(1,2,3,4)) # Creates a list
a$get()
#[1] 1 2 3 4 
a$getmean()
#NULL
cachemean(a) #calculates and set the mean of the Vector "a"
#[1] 2.5
a$getmean()  # this is only to show you that the mean has been stored and does not affect anything
#[1] 2.5
cachemean(a) # as the mean has been calculated previously, this returns the saved one
#[1] 2.5
a$set(c(10,20,30,40)) # Set overrides x and set m <- NULL
a$getmean()
#NULL
cachemean(a)
#[1] 25
cachemean(a)
#getting cached data
#[1] 25
a$get()
#[1] 10 20 30 40
a$setmean(0)  # do NOT call setmean() directly despite it being accessible for the reason you will see next
a$getmean()
#[1] 0      # obviously non-sense since...
a$get()
#[1] 10 20 30 40
cachemean(a)
#[1] 0    # as you can see the call to setmean() effectively corrupted the functioning of the code
a <- makeVector(c(5, 25, 125, 625))
a$get()
#[1] 5 25 125 625
cachemean(a)
#[1] 195
cachemean(a)
#getting cached data
#[1] 195
```
### **cachematrix.R**
```{r}
## Pair of functions that cache the inverse of a matrix.

## This function creates a special "matrix" object that can cache its inverse.
makeCacheMatrix <- function(matrix = matrix()) {
    # store inverse value
    inverse <- NULL
    # set the original matrix and reset inverse
    set <- function(y) {
        matrix <<- y
        inverse <<- NULL
    }
    # get the original matrix
    get <- function() matrix
    # set inverse value
    set_inverse <- function(inv) inverse <<- inv
    # get inverse value
    get_inverse <- function() inverse
    
    # Returns a list of the 4 functions, this list is the special "matrix"
    list(set = set, get = get,
         set_inverse = set_inverse,
         get_inverse = get_inverse)
}


## This function computes the inverse of the special "matrix" returned by 
## makeCacheMatrix above. If the inverse has already been calculated (and 
## the matrix has not changed), then the cachesolve should retrieve the 
## inverse from the cache.

cacheSolve <- function(special_matrix, ...) {
    inverse <- special_matrix$get_inverse()
    if(!is.null(inverse)) {
        message("getting cached data")
        return(inverse)
    }
    data <- special_matrix$get()
    inverse <- solve(data, ...)
    special_matrix$set_inverse(inverse)
    inverse
}


## Unit tests (with expected output) for Programming Assignment 2

source("cachematrix.R")

amatrix = makeCacheMatrix(matrix(c(1,2,3,4), nrow=2, ncol=2))
amatrix$get()         # Returns original matrix
# [,1] [,2]
# [1,]    1    3
# [2,]    2    4

cacheSolve(amatrix)   # Computes, caches, and returns    matrix inverse
# [,1] [,2]
# [1,]   -2  1.5
# [2,]    1 -0.5

amatrix$get_inverse()  # Returns matrix inverse
# [,1] [,2]
# [1,]   -2  1.5
# [2,]    1 -0.5

cacheSolve(amatrix)   # Returns cached matrix inverse using previously computed matrix inverse
# getting cached data
# [,1] [,2]
# [1,]   -2  1.5
# [2,]    1 -0.5

amatrix$set(matrix(c(0,5,99,66), nrow=2, ncol=2)) # Modify existing matrix
cacheSolve(amatrix)   # Computes, caches, and returns new matrix inverse
# [,1] [,2]
# [1,] -0.13333333  0.2
# [2,]  0.01010101  0.0

amatrix$get()         # Returns matrix
# [,1] [,2]
# [1,]    0   99
# [2,]    5   66

amatrix$get_inverse()  # Returns matrix inverse
# [,1] [,2]
# [1,] -0.13333333  0.2
# [2,]  0.01010101  0.0
```
### **best.R**
```{r}
data_dir ="./rprog-data-ProgAssignment3-data/"

library(datasets) # to test valid states

file_path <- function(...){paste(data_dir,...,sep = "/")}
valid_outcomes <- c("heart attack", "heart failure", "pneumonia")


## Return hospital name in that state with lowest 30-day death rate for the 
## given outcome
best <- function(state, outcome) {
    ## Check that state and outcome are valid
    if(!(state %in% state.abb)) {stop("invalid state")}
    if(!(outcome %in% valid_outcomes)) {stop("invalid outcome")}
        
    ## colClasses is mandatory to use which.min() method
    data <- read.csv(file = file_path("outcome-of-care-measures.csv"),
                     colClasses="character")
    ## Also give readable names to the columns
    names(data) <- gsub("\\.\\.\\.",".",names(data))
    names(data) <- gsub("\\.\\.",".",names(data))
    names(data) <- tolower(names(data))
        
    # Format and Paste the outcome to access directly to the target column 
    outcome <- gsub(" ",".",outcome)
    outcome <- paste("hospital.30.day.death.mortality.rates.from",outcome,
                     sep=".")
    
    # Row subets matching state and without "Not Available" values
    data <- data[data$state == state & data[outcome] != "Not Available", ]
    
    # Returns the row in which the outcome has the minium value
    data <- data[which.min(data[ ,outcome]), ]
    hospital_name <- data[ ,"hospital.name"]
    hospital_name
}

#best("TX", "heart attack") # [1] "CYPRESS FAIRBANKS MEDICAL CENTER"
#best("TX", "heart failure") # [1] "FORT DUNCAN MEDICAL CENTER"
#best("MD", "heart attack") # [1] "JOHNS HOPKINS HOSPITAL, THE"
#best("MD", "pneumonia") # [1] "GREATER BALTIMORE MEDICAL CENTER"
#best("BB", "heart attack") # Error in best("BB", "heart attack") : invalid state
#best("NY", "hert attack") # Error in best("NY", "hert attack") : invalid outcome
```
### **rankhospital.R**
```{r}
data_dir ="./rprog-data-ProgAssignment3-data/"

library(datasets) # to test valid states

file_path <- function(...){paste(data_dir,...,sep = "/")}
valid_outcomes <- c("heart attack", "heart failure", "pneumonia")

rankhospital <- function(state,outcome,rank){
    ## Check that state and outcome are valid
    if(!(state %in% state.abb)) {stop("invalid state")}
    if(!(outcome %in% valid_outcomes)) {stop("invalid outcome")}
    
    data <- read.csv(file = file_path("outcome-of-care-measures.csv"),
                     colClasses="character")
    
    ## Also give readable names to the columns
    names(data) <- gsub("\\.\\.\\.",".",names(data))
    names(data) <- gsub("\\.\\.",".",names(data))
    names(data) <- tolower(names(data))
    
    # Format and Paste the outcome to access directly to the target column 
    outcome <- gsub(" ",".",outcome)
    outcome <- paste("hospital.30.day.death.mortality.rates.from",outcome,
                     sep=".")
    
    # Row subets matching state and without "Not Available" values
    data <- data[data$state == state & data[outcome] != "Not Available", ]
    
    # Cast as.double so order function can works properly
    data[,outcome] <- as.double(data[,outcome])
    
    # Order the rows by the outcome column and then by hospital name
    data <- data[ order(data[,outcome],data$hospital.name), ]
    
    # Return data depending on the rank input
    if(rank == "best") {hospital <- data[1,"hospital.name"]}
    else if(rank == "worst") {hospital <- data[nrow(data),"hospital.name"]}
    else {hospital <- data[rank,"hospital.name"]}
    
    hospital
}

rankhospital("TX", "heart failure", 4) # [1] "DETAR HOSPITAL NAVARRO"
rankhospital("MD", "heart attack", "worst") # [1] "HARFORD MEMORIAL HOSPITAL"
rankhospital("MN", "heart attack", 5000) # [1] NA
```
### **rankall.R**
```{r}
data_dir ="./rprog-data-ProgAssignment3-data/"

library(datasets) # to test valid states

file_path <- function(...){paste(data_dir,...,sep = "/")}
valid_outcomes <- c("heart attack", "heart failure", "pneumonia")

rankall <- function(outcome,rank="best"){
    ## Check that state and outcome are valid
    if(!(outcome %in% valid_outcomes)) {stop("invalid outcome")}
    
    data <- read.csv(file = file_path("outcome-of-care-measures.csv"),
                     colClasses="character")
    
    ## Also give readable names to the columns
    names(data) <- gsub("\\.\\.\\.",".",names(data))
    names(data) <- gsub("\\.\\.",".",names(data))
    names(data) <- tolower(names(data))
    
    
    # Format and Paste the outcome to access directly to the target column 
    outcome <- gsub(" ",".",outcome)
    outcome <- paste("hospital.30.day.death.mortality.rates.from",outcome,
                     sep=".")    

    data <- data[,c("hospital.name","state",outcome)]
    names(data) <- c("hospital.name","state","outcome")
    
    hospitals <- character(0)
    states <- character(0)
    outcomes <- double(0)
    
    for(state in unique(data$state)){
        # Filter data by the state and only valid values
        state_data <- data[data$state == state & data$outcome != "Not Available",]
        
        # Cast as.double so order function can works properly
        state_data$outcome <- as.double(state_data$outcome)
        
        # Order the rows by the outcome column and then by hospital name
        state_data <- state_data[ order(state_data$outcome,
                                        state_data$hospital.name), ]
        
        # Return data depending on the rank input
        if(rank == "best") { hospital <- state_data[1,"hospital.name"] }
        else if(rank == "worst") { hospital <- state_data[nrow(state_data),"hospital.name"] }
        else { hospital <- state_data[rank,"hospital.name"] }
        
        hospitals <- c(hospitals,hospital)
        states <- c(states,state)
    }
    
    ranked_hospitals = data.frame(hospitals,states)
    ranked_hospitals <- ranked_hospitals[ order(ranked_hospitals$state) , ]
    names(ranked_hospitals) <- c("hospital","state")
    ranked_hospitals
}


head(rankall("heart attack", 20), 10)
tail(rankall("pneumonia", "worst"), 3)
tail(rankall("heart failure"), 10)
```







## 3. Getting and Cleaning Data
### Navigate working directory
```{r}
setwd("./foldername")              # go down one level
setwd("../")                       # go up one level
setwd("/Users/bruno/foldername")   # absolute match
```



### Create folder if doesn't exist
```{r}
if (!file.exists("foldername")) {
  dir.create("foldername")
}

```



### Get data from internet
```{r}
# csv, tab-delim
download.file(url="https://test.com/dataset.csv",
              destfile="./foldername/destination.csv",
              method=, mode='wb' #for xlsx)   
              
list.files("./foldername)
dataDownloaded <- date()
```


### Read local flat files
```{r}
data <- read.table("./foldername/destination.csv",
                   sep=",",
                   header=TRUE,
                   quote = "",           # no quotes in dataset
                   na.strings = "",      # char that represents missing value     
                   nrows=,               # how many rows to read
                   skip=                # how many lines to skip from top  
                  )


# alternatives
read.csv("./foldername/destination.csv", sep=, header=TRUE)
read.csv2() # for larger datasets
```


### Read Excel files
```{r}
library(xlsx)
data <- read.xlsx("./foldername/destination.xlsx",
                  sheetIndex=1,
                  header=TRUE,
                  colIndex=colIndex,
                  rowIndex=rowIndex
                  )
# subsetting
colIndex <- 2:3
rowIndex <- 1:4

# write back file
write.xlsx(#similar args)

#alternatives
read.xlsx2() #faster, but not good for subsets
package::xlconnect

```


### Read XML files
```{r}
library(XML)
fileUrl <- "https://test.com/dataset.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
rootNode <- xmlRoot

# ------------------------------
# Basic XML navigation
# ------------------------------
xmlName(rootNode)
names(RootNode)
rootNode[[1]]
rootNode[[1]][[1]]

xmlSApply(rootNode, xmlValue) # recursively get every value in every tag of the doc (the content)

# ------------------------------
# Web scraping with XPath
# ------------------------------
/node # top level node
//node # node at any level
node[@attr-name] # node with attribute name
node[@attr-name='bob'] # node with specific attribute name

# get all values with name tag in root node
names <- xpathSApply(rootNode, "//name", xmlValue)
# get all values with price tag in root node
prices <- xpathSApply(rootNode, "//price", xmlValue)
# get scores
scores <- xpathSApply(doc, "//li[@class='score", xmlValue)
# get team names
teams <- xpathSApply(doc, "//li[@class='team-name", xmlValue)

```


### Read JSON files
```{r}
library(jsonlite)
jsonData <- fromJSON("https://api.test.com/dataset")

names(jsonData)
names(jsonData$owner)
jsonData$owner$login

# ------------------------------
# Write R dataframes to JSON
# ------------------------------
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)

# decode, identical to iris
iris2 <- fromJSON(myjson)
head(iris2)
```

### Read mySQL
```{r}
#server-level
library/RMySQL)
ucscDb <- dbConnect(MySQL() user="genome", host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb); #ALWAYS CLOSE CONNECTION

# database-level
hg19 <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu", db="hg19")
allTables <- dbListTables(hg19)
length(allTables)

# table-level
dbListFields(hg19, "table-name")
affyData <- dbReadTable(hg19, "table-name")
head(affyData)


# ------------------------------
# QUERIES
# ------------------------------
# count number of records
dbGetQuery(hg19, "select count(*) from table-name")

# subset data
query <- dbSendQuery(, "select * from table-name where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)
# top 10 records
affyMisSmall <- fetch(query, n=10); dbClearResult(query);
```

### Read HDF5
```{r}
library(hdf5r)
created = h5createFile("example.h5")
created

created = h5createGroup("example.h5", "groupname1")
created = h5createGroup("example.h5", "groupname1/subgroupname1")
h5ls("example.h5")

# write to groups
dataset1 = matrix()
h5write(dataset1, "example.h5", "groupname1")
attr(dataset1, "scale") <- "liter" # metadata naming

# write dataset to root
h5write(df, "example.h5", "df") # last arg is var name, not group name

# read
readA = h5read("example.h5", "foo/foobaa/B")

# write/read chunks
h5write(c(12,13,14), "example.h5", "foo/A", index=list(1:3,1))
h5read("example.h5", "foo/A", index=list(1:3,1))
```


### Read from the Web (Webscraping)
```{r}
con = url("http://url.com")
htmlCode = readLines(con)
close(con)
htmlCode

# ------------------------------
# Using GET from httr
# ------------------------------
library(httr)
html2 = GET(url)
content2 = content(html2, as="text")
parsedHtml = htmlParse(content2, asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)


# ------------------------------
# Acessing websites with passwords
# ------------------------------
pg2 = GET(url, authenticate("user", "password")) # status 401 = fail, 200 = success
content3 = content(pg2, as="text")
...


# ------------------------------
# Using handles (avoid keeping to authenticate)
# ------------------------------
google = handle("http://google.com")
pg1 = GET(handle=google, path="/")
pg2 = GET(handle=google, path="search") # "save" cookies in the handle
```


### Read from APIs
```{r}
myapp = oauth_app("twitter", key="yourConsumerKeyHere", secret="yourConsumerSecretHere")
sig = sign_oauth1.0(myapp, token="yourTokenHere", token_secret="yourTokenSecretHere")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)

# ------------------------------
# Re-format data object as data frame for readability
# ------------------------------
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2
```


### Reading other sources
```{r}
file()
url()
gzfile()
bzfile()
?connections

# ------------------------------
# FOREIGN package
# ------------------------------
read.arff    # Weka
read.dta     # Stata
read.mtp     # Minitab
read.octave  # Octave
read.spss    # SPSS
read.xport   # SAS


# ------------------------------
# databases
# ------------------------------
package::RPostgreSQL
package::RODBC
package::Rmongo
package::rmongodb


# ------------------------------
# images
# ------------------------------
jpeg
readbitmap
png
EBImage

# ------------------------------
# GIS (geographic) data
# ------------------------------
rdgal
rgeos
raster

# ------------------------------
# music
# ------------------------------
tuneR
seewave
```


### data.table Package
More efficient than data frame
```{r}
library(data.table)
dt = data.table(x=rnorm(9), y=rep(c("a", "b", "c"), each = 3), z=rnorm(9))

tables() # check all data tables in memory

# ------------------------------
# HOW TO USE
# ------------------------------
dt[2,]
dt[dt$y=="a"]

# subset rows
dt[c(2,3)]

# subset cols, use expressions | NOT dt[,c(2,3)]
{
  x=1
  y=2
}
k = {print(10); 5} # 10
print(k)           # 5

# pass functions to expressions
dt[,list(mean(x), sum(z))]
dt[,table(y)]

# add new columns :=
dt2 = copy(dt) # dt2 <- dt doesnt work
dt[,w:z^2]

# multi step operations to a new column
dt[,m:={tmp <- (x+z);log2(tmp+5)}]

# plyr logical (TRUE/FALSE in new col)
dt[,a:=x>0]

# plyr group by logical 'a'
dt[,b:=mean(x+w),by=a]

# .N counts number of times value appears
dt[, .N, by=x]

# keys to rapidly subset a table
dt <- data.table(x=rep(c("a", "b", "c"), each=100), y=rnorm(300))
setkey(dt, x)
dt['a']

# JOINS
DT1 <- data.table(x=c('a','a','b','dt1'), y=1:4)
DT1 <- data.table(x=c('a','b','dt2'), z=5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1,DT2) # use key to merge

# FAST READING WITH TEMP FILES
big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names=FALSE, col.names=TRUE, sep="\t", quote=FALSE)

system.time(fread(file)) # .326 s
system.time(read.table(file,header=TRUE, sep="\t")) # 5.7 s
```

### Subsetting and sorting
```{r}
#if/or statements in dataframe X
X[(condition & condition),]
X[(condition | condition),]

# return indices were condition is met. good for not subsetting NAs
X[which(X$var1 > 8),]

# sorting (e.g. sorts column)
sort(X$var1)
sort(X$var1, decreasing=TRUE)
sort(X$var1, na.last=TRUE) # put NA at end of sort

# ordering (e.g. sorts entire dataframe with respect to column)
X[order(X$var1),]
X[order(X$var1, X$var3),]

library(plyr)
arrange(X, var1) # ascending
arrange(X, desc(var1)) # descending

# add data
X$var4 <- rnorm(5) # simply assign new col name)
Y <- cbind(X, rnorm(5)) # cbind is not permanent, so store in var
```


### Summarizing data
```{r}
head(dataset,n=3)
tail(dataset,n=3)
summary(dataset)
str(dataset)
quantile(dataset$col1, na.rm=TRUE)
quantile(dataset$col1, probs=c(0.5,0.75,0.9))
table(dataset$col1, useNA="ifany") # useNA reports amount of NAs in sep col
table(dataset$col1, dataset$col2)

# amount of NAs in column
sum(is.na(dataset$col1))

# logical that reports any NAs
any(is.na(dataset$col1))

# logical check if all verifies the condition
all(dataset$col1 > 0)

# col/row manipulation
colSums(is.na(dataset))
rowSums(dataset)

# checks if any values are 21212, same as == but for multiple checks
table(dataset$zipcode %in% c("21212"))
table(dataset$zipcode %in% c("21212", "21213"))

# subset
dataset[dataset$zipcode %in% c("21212", "21213"),]

# cross tabs
data(UCBAdmissions)
DF = as.data.frame(UCBAdmissions)
xt <- xtabs(Freq ~ Gender+Admit, data=DF) # Freq: data that shows as values
                                          # Gender+Admit: vars you want to break
                                          # down your values by

# break by all variables
xtabs(breaks ~ ., data=warpbreaks)

# flat tables (more compact cross tabs for large tables)
ftable(xt)

# check size of dataset
object.size(dataset)
print(object.size(dataset), units="Mb")
```


### Creating new variables
```{r}
# sequences useful for index
dataset$newvar = seq(min, max, by=step)
dataset$newvar = seq(min, max, length=int)
dataset$newvar = seq(along=c(1,3,8,25,100))

# create binary variables
restData$zipWrong = ifelse(restData$zipCode < 0, TRUE, FALSE)
table(restData$zipWrong, restData$zipCode < 0)

# create categorical variables
restData$zipGroups = cut(restData$zipCode, breaks=quantile(restData$zipCode)) # create bins defined by quartile intervals (they are factors now)
table(restData$zipGroups)

# easier cutting
library(Hmisc)
restData$zipGroups = cut2(restData$zipCode, g=4)
table(restData$zipGroups)


# create factor variables
restData$zipcodeFactor <- factor(restData$zipCode)

# levels of factor variables
yesno <- sample(c("yes", "no"), size=10, replace=TRUE)   # create mock list
yesnofac = factor(yesno, levels=c("yes", "no"))          # custom order
relevel(yesnofac, ref="no")                              # to force 1st factor assignment
as.numeric(yesnofac)                                     # numeric convertible

# mutate function: create var + add to dataset simultaneously
library(plyr); library(Hmisc)
restData2 = mutate(restData, zipGroups=cut2(zipCode,g=4))

# ------------------------------
# COMMON TRANSFORMS
# ------------------------------
abs(x)
sqrt(x)
ceiling(3.475) # round up: 4
floor(3.475) # round down: 3
round(3.475, digits=2) # round decimals: 3.48
signif(3.475, digits=2) # amount of significant digits: 3.5
cos(x); sin(x); tan(x)
log2(x); log10(x); log(x) #log e. use logs when data is very skewed/contains lots of outliers
exp(x)
```



### Reshaping data
```{r}
library(reshape2)

# take mtcars dataset, get id columns, create new cols that contain mpg, hp, and respective values
carMelt <- melt(mtcars, id=c("carname", "gear", "cyl"), measure.vars=c("mpg", "hp"))

# put dataset back together, broken down by a variable
cylData <- dcast(carMelt, cyl ~ variable) # default is a count: e.g. for 4 cylinders, we have 11 measures of mpg and hp
cylData <- dcast(carMelt, cyl ~ variable,mean) # get mean

# averaging values
tapply(InsectSprays$count, InsectSprays$spray, sum)

# Split-apply-combine
# split
spIns = split(InsectSprays$count, InsectSprays$spray)
# apply
sprCount = lapply(spIns, sum)
# combine
unlist(sprCount)


# using plyr for same result
ddply(InsectSprays,.(spray),summarize,sum=sum(count))
# get sum but maintain equal dims
ddply(InsectSprays,.(spray), summarize, sum=ave(count,FUN=sum))


# others
acast()     # dcast for data.frame, acast to turn into array
arrange()   # faster reordering without using order()
mutate()    # add new variables
```
### Merging data
```{r}
# check by what columns can the datasets be merged
intersect(names(dataset1), names(dataset2))

# match datasets based on id (default is an outer join)
merge(x, y, by, by.x, by.y, all)
merge(dataset1, dataset2, by.x="solution_id", by.y="id", all=TRUE)


# using plyr (default is left join)
df1 = data.frame(id=sample(1:10), x=rnorm(10))
df2 = data.frame(id=sample(1:10), y=rnorm(10))
arrange(join(df1,df2), id) # can only merge by one var

df1 = data.frame(id=sample(1:10), x=rnorm(10))
df2 = data.frame(id=sample(1:10), y=rnorm(10))
df3 = data.frame(id=sample(1:10), z=rnorm(10))
dfList = list(df1,df2,df3)
join_all(dfList)
```

### DPLYR package
```{r}
arrange()     # reorder rows
filter()      # return row subset based on logic
select()      # return column subset
mutate()      # add new variables/columns, transform existing
rename()      # rename variables
summarize()   # generate summary statistics


bind_rows()
bind_cols()

# ------------------------------
# DPLYR usage example, full walkthrough
# ------------------------------
library(dplyr)
chicago <- readRDS("chicago.rds")
# chicagoDF <- tbl_df(chicago)
dim(chicago)
str(chicago)
names(chicago)


head(select(chicago, city:dptp)) # get these cols
head(select(chicago, -(city:dptp))) # get all cols except these ones


chic.f <- filter(chicago, pm25tmean2 > 30 & tmpd > 80)
head(chic.f)


chicago <- arrange(chicago, date)
head(chicago)
tail(chicago)


chicago <- arrange(chicago, desc(date))
head(chicago)
tail(chicago)


chicago <- rename(chicago, pm25 = pm25tmean2, dewpoint = dptp)
head(chicago)


chicago <- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm=TRUE))
head(select(chicago, pm25, pm25detrend))


chicago <- mutate(chicago, tempcat = factor(1 * (tmpd > 80), labels = c("cold", "hot")))
hotcold <- group_by(chicago, tempcat)
head(hotcold)
summarize(hotcold, pm25 = mean(pm25, na.rm=TRUE), o3 = max(o3tmean2), no2 = median(no2tmean2))


chicago <- mutate(chicago, year = as.POSIXlt(date)$year + 1900)
years <- group_by(chicago, year)
summarize(years, pm25 = mean(pm25, na.rm=TRUE), o3 = max(o3tmean2), no2 = median(no2tmean2))


# put it all together using the pipeline operator %>% (avoids variable assignment at each step)
chicago %>% mutate(month = as.POSIXlt(date)$mon + 1) %>% group_by(month) %>% summarize(years, pm25 = mean(pm25, na.rm=TRUE), o3 = max(o3tmean2), no2 = median(no2tmean2))
```





### TIDYR/READR package
http://vita.had.co.nz/papers/tidy-data.pdf
```{r}
# PROBLEM 1: you have column headers that are values, not variable names.
gather(students, sex, count, -grade)


# PROBLEM 2: multiple variables are stored in one column.
gather(students2, sex_class, count, -grade)
separate(res, sex_class, c("sex", "class"))


# PROBLEM 3: variables are stored in both rows and columns.
students3 %>%
  gather(class, grade, class1:class5, na.rm = TRUE) %>%
  spread(test, grade) %>%
  mutate(class=parse_number(class)) %>%
  print


# PROBLEM 4: multiple observational units are stored in the same table.
student_info <- students4 %>%
  select(id, name, sex) %>%
  unique() %>%
  print

gradebook <- students4 %>%
  select(id, class, midterm, final) %>%
  print


# PROBLEM 5: a single observational unit is stored in multiple tables (opposite of problem 4).
passed <- passed %>% mutate(status="passed")
failed <- failed %>% mutate(status="failed")
bind_rows(passed, failed)


# ------------------------------
# WALKTHROUGH OF REAL EXAMPLE
# ------------------------------
sat %>%
  select(-contains("total")) %>%   # contains() checks partial string in string
  gather(part_sex, count, -score_range) %>%
  separate(part_sex, c("part", "sex")) %>%
  group_by(part, sex) %>%
  mutate(total=sum(count),
         prop = count/total
  ) %>% print
```


### Editing malformatted text
```{r}
tolower()
toupper()

# split on period
strsplit(string, "\\.")

# keep only first element of the split
firstElement <- function(x){x[1]}
sapply(splitNames,firstElement)

# substitute underscore by space
sub("_", " ", names(reviews),)

# substitute multiple underscores
gsub("_", " ", string)

# find strings by index or values
grep("string", dataset$column)
grep("string", dataset$column, value=TRUE)

# find strings by logic
table(grepl("string", dataset$column))

# subset by found string
subset <- dataset[!grepl("string", dataset$column),]
```
### STRINGR package
```{r}
library(stringr)

# number of characters in the string
nchar("string")

# subset string from n to m
substr("string", n, m)

# paste strings together
paste("string1", "string2", sep=" ")
paste0("string1", "string2")

# trim spaces in strings
str_trim("string    ")
```




### Regular expressions
```{r}
# metacharacters
^string                         # start of line
string$                         # end of line
  
[Ss][Tt][Rr][Ii][Nn][Gg]        # case insensitive matching of word 'string'
^[0-9][a-zA-Z]                  # range of numbers + letters

[^ ]                            # NOT a space
[^?.]$                          # search end of line that DOESN'T end with ? or .
9.11                            # . refers to any character

flood|fire|hurricane            # | refers to either flood OR fire OR hurricane
[Gg]eorge( [Ww]\.)? [Bb]ush     # ? denotes stuff in () is optional in George W. Bush. the \ ensures that . is a LITERAL, not metacharacter

(.*)                            # ANY character, ANY number of times
(.+)                            # ANY character, at least ONCE

^s(.*)s$                        # the * searches for longest possible string that starts and ends with 's'
^s(.*?)s$                       # deactivates length greediness of *
{1, 5}                          # interval quantifiers: between 1 and 5 of something

m,n                             # between m and n matches
m                               # exactly m matches
m,                              # at least m matches

 +([a-zA-Z]+) +\1 +             # replicate = 1, find repeated strings in sequence: "blah blah"

# COMPLEX CASES
^[Gg]ood|[Bb]ad                 # start of line good, or bad anywhere, case insensitive
[0-9]+ (.*)[0-9]+               # at least one number, any character in () and at least one number
[Bb]ush( +[^]+ +){1,5} debate   # 1 to 5 words between B/bush and debate
```


### Working with dates
```{r}
d1 = date()        # character class
d2 = Sys.Date()    # date class

format(Sys.Date(), "%a %b %d") # abbreviated weekday, month, day
format(Sys.Date(), "%A %B %D") # unabbreviated weekday, month, day

x = c("1jan1960","30jul2011"); z = as.Date(x, "%d%b%Y")
diff = z[1]-z[2] # time difference in days
as.numeric(diff)

weekdays(d2)
months(d2)
julian(d2) # days since the 'origin'
```


### LUBRIDATE package
```{r}
library(lubridate)

ymd("20140108") # year month date conversion
mdy("08/04/2013")
dmy("03-04-2013")
ymd_hms("2011-08-03 10:15:03", tz="Europe/London")


wday(d2)
wday(d2,label=TRUE)
```

### **run_analysis.R**
```{r}
# ---------------------------
# imports
# ---------------------------
library(data.table)
library(reshape2)


# ---------------------------
# download and unzip dataset
# ---------------------------
path <- getwd()
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
download.file(url, file.path(path, "dataset.zip"))
unzip(zipfile = "dataset.zip")



# ---------------------------
# UCI HAR Dataset folder: loads
# ---------------------------
activityLabels <- fread(file.path(path, "UCI HAR Dataset/activity_labels.txt"), col.names=c("Label", "Activity"))
features <- fread(file.path(path, "UCI HAR Dataset/features.txt"), col.names=c("id", "featureName"))

# filter only parameters that are mean or std
featuresWanted <- grep("(mean|std)\\()", features$featureName)

# clean up names
measurements <- gsub('[()]', '', featuresWanted)



# ---------------------------
# TRAIN dataset
# ---------------------------
# load training values
train <- fread(file.path(path, "UCI HAR Dataset/train/X_train.txt"))[, featuresWanted, with = FALSE]

# change column names to each measurement
setnames(train, colnames(train), measurements)

# load type of activity for those specific metrics
trainActivities <- fread(file.path(path, "UCI HAR Dataset/train/Y_train.txt"), col.names = c("Activity"))

# load person id using the wearable
trainSubjects <- fread(file.path(path, "UCI HAR Dataset/train/subject_train.txt"), col.names = c("Subject"))

# merge all data into the train set
train <- cbind(trainSubjects, trainActivities, train)


# ---------------------------
# TEST dataset
# ---------------------------
# load testing values
test <- fread(file.path(path, "UCI HAR Dataset/test/X_test.txt"))[, featuresWanted, with = FALSE]

# change column names to each measurement
setnames(test, colnames(test), measurements)

# load type of activity for those specific metrics
testActivities <- fread(file.path(path, "UCI HAR Dataset/test/Y_test.txt"), col.names = c("Activity"))
# load person id using the wearable
testSubjects <- fread(file.path(path, "UCI HAR Dataset/test/subject_test.txt"), col.names = c("Subject"))

# merge all data into the test set
test <- cbind(testSubjects, testActivities, test)



# ---------------------------
# MERGE train and test data
# ---------------------------
traintest <- rbind(train, test)


# ---------------------------
# Change activity and subject 
# values to descriptive labels
# ---------------------------
traintest[["Activity"]] <- factor(traintest[, Activity], levels = activityLabels[["Label"]], labels = activityLabels[["Activity"]])
traintest[["Subject"]] <- as.factor(traintest[, Subject])


# ---------------------------
# Secondary dataset with 
# mean by subject and activity
# ---------------------------

# group by subject and activity
traintest_mean <- melt(traintest, c("Subject", "Activity"))

# create new dataset
traintest_mean <- dcast(traintest_mean, Subject+Activity ~ variable, fun.aggregate = mean)

# assign descriptive column names
traintest_labels <- c("Subject", "Activity", grep("(mean|std)\\()", features$featureName, value=TRUE))
names(traintest_mean) <- traintest_labels

# write dataset to tidyData.txt
fwrite(traintest_mean, file="tidyData.txt", quote=FALSE)
```



## 4. Exploratory Data Analysis